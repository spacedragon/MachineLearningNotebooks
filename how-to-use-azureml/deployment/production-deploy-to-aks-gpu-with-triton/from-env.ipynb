{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying a web service hosted on NVIDIA Triton to Azure Kubernetes Service (ACI)\n",
        "This notebook shows the steps for deploying a service with [NVIDIA Triton Inferencing Server](https://developer.nvidia.com/nvidia-triton-inference-server): registering a model, creating an image, provisioning a cluster (one time action), and deploying a service to it. \n",
        "We then test and delete the service, image and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.10.0\n"
        }
      ],
      "source": [
        "import azureml.core\n",
        "print(azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get workspace\n",
        "Load existing workspace from the config file info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING - Warning: Falling back to use azure cli login credentials.\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\nPlease refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\nyultest\nyultest\ncentraluseuap\na5fe3bc5-98f0-4c84-affc-a589f54d9b23\n"
        }
      ],
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"a5fe3bc5-98f0-4c84-affc-a589f54d9b23\")\n",
        "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"yultest\")\n",
        "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"yultest\")\n",
        "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"centraluseuap\")\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download the model\n",
        "\n",
        "Prior to registering the model, you should have a model in one of the [supported formats](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#framework-model-definition) by Triton. This cell will download a [pretrained ONNX densenet](https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx) model.\n",
        "\n",
        "** Note: ** If you have a previously-registered model, the file name needs to follow the [naming convention](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html?highlight=model%20onnx#framework-model-definition) or be specified using model configuration file below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "model_url = 'https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx'\n",
        "\n",
        "version = '1'\n",
        "model_dir = os.path.join('models', 'triton', 'densenet_onnx', version)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "target_file = os.path.join(model_dir, 'model.onnx')\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    response = requests.get(model_url)\n",
        "    open(target_file, 'wb').write(response.content)\n",
        "\n",
        "config_file = os.path.join('models', 'triton', 'densenet_onnx', 'config.pbtxt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Model Configuration file\n",
        "\n",
        "Each model needs a [Model Configuration](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_configuration.html) that provides required and optional information about the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing models/triton/densenet_onnx/config.pbtxt\n"
        }
      ],
      "source": [
        "%%writefile $config_file\n",
        "name: \"densenet_onnx\"\n",
        "platform: \"onnxruntime_onnx\"\n",
        "max_batch_size: 0\n",
        "input [\n",
        "  {\n",
        "    name: \"data_0\"\n",
        "    data_type: TYPE_FP32\n",
        "    format: FORMAT_NCHW\n",
        "    dims: [ 3, 224, 224 ]\n",
        "    reshape { shape: [ 1, 3, 224, 224 ] }\n",
        "  }\n",
        "]\n",
        "output [\n",
        "  {\n",
        "    name: \"fc6_1\"\n",
        "    data_type: TYPE_FP32\n",
        "    dims: [ 1000 ]\n",
        "    reshape { shape: [ 1, 1000, 1, 1 ] }\n",
        "    label_filename: \"densenet_labels.txt\"\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register the model\n",
        "Register an existing trained model, add description and tags.\n",
        "\n",
        "** Note: ** Under `model_path` there must be a sub-directory named `triton`, which has the structure of a Triton [Model Repository](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#repository-layout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model densenet_onnx\ndensenet_onnx Image classification trained on Imagenet Dataset 2\n"
        }
      ],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path=\"models\", # This points to the local directory to upload.\n",
        "                       model_name=\"densenet_onnx\", # This is the name the model is registered as.\n",
        "                       tags={'area': \"Image classification\", 'type': \"classification\"},\n",
        "                       description=\"Image classification trained on Imagenet Dataset\",\n",
        "                       workspace=ws)\n",
        "\n",
        "print(model.name, model.description, model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy the model as a web service to ACI\n",
        "\n",
        "First create a scoring script\n",
        "\n",
        "** Note: ** Triton server listens to a fixed local port. You may choose to use the Triton Python [client library](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/client_library.html) to talk to it, while keeping the flexibility of pre-/post- processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting score.py\n"
        }
      ],
      "source": [
        "%%writefile score.py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "from functools import partial\n",
        "import os\n",
        "import io\n",
        "\n",
        "import tritonhttpclient\n",
        "from tritonclientutils import InferenceServerException\n",
        "\n",
        "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
        "from azureml.contrib.services.aml_response import AMLResponse\n",
        "\n",
        "sys.path.append(os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'models'))\n",
        "from utils import preprocess, postprocess\n",
        "\n",
        "\n",
        "trition_client = None\n",
        "_url = \"localhost:8000\"\n",
        "_model = \"densenet_onnx\"\n",
        "_scaling = \"INCEPTION\"\n",
        "\n",
        "def init():\n",
        "    global triton_client, max_batch_size, input_name, output_name, dtype\n",
        "\n",
        "    triton_client = tritonhttpclient.InferenceServerClient(_url)\n",
        "\n",
        "    max_batch_size = 0\n",
        "    input_name = \"data_0\"\n",
        "    output_name = \"fc6_1\"\n",
        "    dtype = \"FP32\"\n",
        "\n",
        "\n",
        "@rawhttp\n",
        "def run(request):\n",
        "    if request.method == 'POST':\n",
        "        \n",
        "        reqBody = request.get_data(False)\n",
        "        img = Image.open(io.BytesIO(reqBody))\n",
        "        \n",
        "        image_data = preprocess(img, _scaling, dtype)\n",
        "        \n",
        "        input = tritonhttpclient.InferInput(input_name, image_data.shape, dtype)\n",
        "        input.set_data_from_numpy(image_data, binary_data=True)\n",
        "        output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True, class_count=1)\n",
        "    \n",
        "        res = triton_client.infer(_model,\n",
        "                                [input],\n",
        "                                request_id=\"0\",\n",
        "                                outputs=[output])\n",
        "\n",
        "        result = postprocess(res, output_name, 1, max_batch_size > 0)\n",
        "\n",
        "        return AMLResponse(result, 200)\n",
        "    else:\n",
        "        return AMLResponse(\"bad request\", 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "# env = Environment.get(workspace=ws, name=\"AzureML-Triton-20.07\")\n",
        "env = Environment.load_from_directory(path = \"./myenv\")\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the deployment configuration objects and deploy the model as a webservice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set the web service configuration (using default here)\n",
        "from azureml.core import Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.exceptions import WebserviceException\n",
        "\n",
        "\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
        "aci_config = AciWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running..............................................................................................................................................\nSucceeded\nACI service creation operation finished, operation \"Succeeded\"\nHealthy\nCPU times: user 1.52 s, sys: 167 ms, total: 1.69 s\nWall time: 13min 29s\n"
        }
      ],
      "source": [
        "%%time\n",
        "service_name ='densenet-onnx'\n",
        "\n",
        "try:\n",
        "    Webservice(ws, service_name).delete()\n",
        "except WebserviceException:\n",
        "    pass\n",
        "\n",
        "service = Model.deploy(workspace=ws,\n",
        "                           name=service_name,\n",
        "                           models=[model],\n",
        "                           inference_config=inference_config,\n",
        "                           deployment_config=aci_config)\n",
        "\n",
        "service.wait_for_deployment(show_output = True)\n",
        "print(service.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "service.get_logs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test the web service\n",
        "We test the web sevice by passing the test images content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "12.989695 (817) = SPORTS CAR\nCPU times: user 5.15 ms, sys: 139 µs, total: 5.29 ms\nWall time: 227 ms\n"
        }
      ],
      "source": [
        "%%time\n",
        "import requests\n",
        "\n",
        "# if (key) auth is enabled, fetch keys and include in the request\n",
        "\n",
        "\n",
        "# # if token auth is enabled, fetch token and include in the request\n",
        "# access_token, fetch_after = aks_service.get_token()\n",
        "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
        "\n",
        "test_sample = open('car.jpg', 'rb').read()\n",
        "resp = requests.post(service.scoring_uri, test_sample)\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean up\n",
        "Delete the service, image, model and compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "service.delete()\n",
        "model.delete()\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "vaidyas"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('myenv': conda)",
      "language": "python",
      "name": "python_defaultSpec_1599023848549"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}